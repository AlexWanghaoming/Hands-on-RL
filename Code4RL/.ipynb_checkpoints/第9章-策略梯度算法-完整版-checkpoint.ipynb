{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第9章 策略梯度算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 简介\n",
    "\n",
    "本书之前介绍的 Q-learning、DQN 及 DQN 改进算法都是**基于价值**（value-based）的方法，其中 Q-learning 是处理有限状态的算法，而 DQN 可以用来解决连续状态的问题。在强化学习中，除了基于值函数的方法，还有一支非常经典的方法，那就是**基于策略**（policy-based）的方法。对比两者，基于值函数的方法主要是学习值函数，然后根据值函数导出一个策略，学习过程中并不存在一个显式的策略；而基于策略的方法则是直接显式地学习一个目标策略。策略梯度是基于策略的方法的基础，本章从策略梯度算法说起。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 策略梯度\n",
    "\n",
    "基于策略的方法首先需要将策略参数化。假设目标策略$\\pi_\\theta$是一个随机性策略，并且处处可微，其中$\\theta$是对应的参数。我们可以用一个线性模型或者神经网络模型来为这样一个策略函数建模，输入某个状态，然后输出一个动作的概率分布。我们的目标是要寻找一个最优策略并最大化这个策略在环境中的期望回报。我们将策略学习的目标函数定义为\n",
    "$$\n",
    "J(\\theta)= \\mathbb{E}_{s_0}[V^{\\pi_\\theta}(s_0)]\n",
    "$$\n",
    "其中，$s_0$表示初始状态。现在有了目标函数，我们将目标函数对策略$\\theta$求导，得到导数后，就可以用梯度上升方法来最大化这个目标函数，从而得到最优策略。\n",
    "\n",
    "我第3章讲解过策略$\\pi$下的状态访问分布，在此用$\\nu^{\\pi}$表示。然后我们对目标函数求梯度，可以得到如下式子，更详细的推导过程将在9.6节给出。\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta}J(\\theta)\n",
    "&\\propto \\sum_{s \\in S}\\nu^{\\pi_\\theta}(s)\\sum_{a \\in A}Q^{\\pi_\\theta}(s,a)\\nabla_{\\theta}\\pi_{\\theta}(a|s)\\\\\n",
    "&=\\sum_{s \\in S}\\nu^{\\pi_\\theta}(s)\\sum_{a \\in A}\\pi_{\\theta}(a|s)Q^{\\pi_\\theta}(s,a)\\frac{\\nabla_{\\theta}\\pi_{\\theta}(a|s)}{\\pi_{\\theta}(a|s)}\\\\\n",
    "&= \\mathbb{E}_{\\pi_\\theta}[Q^{\\pi_\\theta}(s,a)\\nabla_{\\theta}\\log \\pi_{\\theta}(a|s)]\n",
    "\\end{align*}\n",
    "$$\n",
    "这个梯度可以用来更新策略。需要注意的是，因为上式中期望$\\mathbb{E}$的下标是$\\pi_\\theta$，所以策略梯度算法为在线策略（on-policy）算法，即必须使用当前策略$\\pi_\\theta$采样得到的数据来计算梯度。直观理解一下策略梯度这个公式，可以发现在每一个状态下，梯度的修改是让策略更多地去采样到带来较高$Q$值的动作，更少地去采样到带来较低$Q$值的动作，如图9-1所示。\n",
    "\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/r0gkwv61ts.JPG?imageView2/0/w/640/h/640)\n",
    "<center>图9-1 策略梯度示意图</center>\n",
    "\n",
    "\n",
    "在计算策略梯度的公式中，我们需要用到$Q^{\\pi_\\theta}(s,a)$，可以用多种方式对它进行估计。接下来要介绍的 REINFORCE 算法便是采用了蒙特卡洛方法来估计$Q^{\\pi_\\theta}(s,a)$，对于一个有限步数的环境来说，REINFORCE 算法中的策略梯度为：\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta}J(\\theta) = \\mathbb{E}_{\\pi_\\theta}\\left[ \\sum_{t=0}^T \\left( \\sum_{t'=t}^{T}\\gamma^{t'-t}r_{t'} \\right)\\nabla_{\\theta}\\log \\pi_{\\theta}(a_t|s_t) \\right]\n",
    "\\end{align*}\n",
    "$$\n",
    "其中，$T$是和环境交互的最大步数。例如，在车杆环境中，$T=200$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 REINFORCE\n",
    "REINFORCE 算法的具体算法流程如下：\n",
    "\n",
    "- 初始化策略参数$\\theta$\n",
    "- **for** 序列 $e = 1 \\rightarrow E$ **do** : \n",
    "- $\\quad\\quad$ 用当前策略$\\pi_\\theta$采样轨迹$\\{s_{1},a_{1},r_{1},s_{2},a_{2},r_{2}, \\ldots s_{T},a_{T},r_{T}\\}$\n",
    "- $\\quad\\quad$ 计算当前轨迹每个时刻$t$往后的回报$\\sum_{t'=t}^{T}\\gamma^{t'-t}r_{t'}$，记为$\\psi_{t}$\n",
    "- $\\quad\\quad$ 对$\\theta$进行更新，$\\theta = \\theta + \\alpha \\sum_t^T \\psi_{t}\\nabla_{\\theta}\\log \\pi_{\\theta}(a_{t}|s_{t})$\n",
    "- **end for**\n",
    "\n",
    "\n",
    "这便是 REINFORCE 算法的全部流程了。接下来让我们来用代码来实现它，看看效果如何吧！"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4 REINFORCE 代码实践\n",
    "\n",
    "我们在车杆环境中进行 REINFORCE 算法的实验。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import rl_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先定义策略网络`PolicyNet`，其输入是某个状态，输出则是该状态下的动作概率分布，这里采用在离散动作空间上的`softmax()`函数来实现一个可学习的**多项分布**（multinomial distribution）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return  F.softmax(self.fc2(x),dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再定义我们的 REINFORCE 算法。在函数`take_action()`函数中，我们通过动作概率分布对离散的动作进行采样。在更新过程中，我们按照算法将损失函数写为策略回报的负数，即$-\\sum_t \\psi_{t}\\nabla_{\\theta}\\log \\pi_{\\theta}(a_{t}|s_{t})$，对$\\theta$求导后就可以通过梯度下降来更新策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE:\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, learning_rate, gamma, \n",
    "            device):\n",
    "        self.policy_net = PolicyNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.optimizer= torch.optim.Adam(self.policy_net.parameters(), \n",
    "            lr=learning_rate) # 使用Adam优化器\n",
    "        self.gamma = gamma # 折扣因子\n",
    "        self.device = device\n",
    "\n",
    "    def take_action(self, state): # 根据动作概率分布随机采样\n",
    "        state = torch.tensor([state], dtype=torch.float).to(self.device)\n",
    "        probs = self.policy_net(state)\n",
    "        action_dist = torch.distributions.Categorical(probs)\n",
    "        action = action_dist.sample()\n",
    "        return action.item()\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        reward_list = transition_dict['rewards']\n",
    "        state_list = transition_dict['states']\n",
    "        action_list = transition_dict['actions']\n",
    "\n",
    "        G = 0\n",
    "        self.optimizer.zero_grad()\n",
    "        for i in reversed(range(len(reward_list))): # 从最后一步算起\n",
    "            reward = reward_list[i]\n",
    "            state = torch.tensor([state_list[i]], dtype=torch.float).to(self.device)\n",
    "            action = torch.tensor([action_list[i]]).view(-1, 1).to(self.device)\n",
    "            log_prob = torch.log(self.policy_net(state).gather(1, action))\n",
    "            G = self.gamma * G + reward\n",
    "            loss = - log_prob * G # 每一步的损失函数\n",
    "            loss.backward() # 反向传播计算梯度\n",
    "        self.optimizer.step() # 梯度下降"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义好策略，我们就可以开始实验了，看看 REINFORCE 算法在车杆环境上表现如何吧！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "num_episodes = 1000\n",
    "hidden_dim = 128\n",
    "gamma = 0.98\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "env_name = \"CartPole-v0\"\n",
    "env = gym.make(env_name)\n",
    "env.seed(0)\n",
    "torch.manual_seed(0)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "agent = REINFORCE(state_dim, hidden_dim, action_dim, learning_rate, gamma, device)\n",
    "\n",
    "return_list = []\n",
    "for i in range(10):\n",
    "    with tqdm(total=int(num_episodes/10), desc='Iteration %d' % i) as pbar:\n",
    "        for i_episode in range(int(num_episodes/10)):\n",
    "            episode_return = 0\n",
    "            transition_dict = {'states': [], 'actions': [], 'next_states': [], \n",
    "                    'rewards': [], 'dones': []}\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = agent.take_action(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                transition_dict['states'].append(state)\n",
    "                transition_dict['actions'].append(action)\n",
    "                transition_dict['next_states'].append(next_state)\n",
    "                transition_dict['rewards'].append(reward)\n",
    "                transition_dict['dones'].append(done)\n",
    "                state = next_state\n",
    "                episode_return += reward\n",
    "            return_list.append(episode_return)\n",
    "            agent.update(transition_dict)\n",
    "            if (i_episode+1) % 10 == 0:\n",
    "                pbar.set_postfix({'episode': '%d' % (num_episodes/10 * i + \n",
    "                        i_episode+1), 'return': '%.3f' % np.mean(return_list[-10:])})\n",
    "            pbar.update(1)\n",
    "\n",
    "# Iteration 0: 100%|██████████| 100/100 [00:04<00:00, 23.88it/s, episode=100, \n",
    "# return=55.500]\n",
    "# Iteration 1: 100%|██████████| 100/100 [00:08<00:00, 10.45it/s, episode=200, \n",
    "# return=75.300]\n",
    "# Iteration 2: 100%|██████████| 100/100 [00:16<00:00,  4.75it/s, episode=300, \n",
    "# return=178.800]\n",
    "# Iteration 3: 100%|██████████| 100/100 [00:20<00:00,  4.90it/s, episode=400, \n",
    "# return=164.600]\n",
    "# Iteration 4: 100%|██████████| 100/100 [00:21<00:00,  4.58it/s, episode=500, \n",
    "# return=156.500]\n",
    "# Iteration 5: 100%|██████████| 100/100 [00:21<00:00,  4.73it/s, episode=600, \n",
    "# return=187.400]\n",
    "# Iteration 6: 100%|██████████| 100/100 [00:22<00:00,  4.40it/s, episode=700, \n",
    "# return=194.500]\n",
    "# Iteration 7: 100%|██████████| 100/100 [00:23<00:00,  4.24it/s, episode=800, \n",
    "# return=200.000]\n",
    "# Iteration 8: 100%|██████████| 100/100 [00:23<00:00,  4.33it/s, episode=900, \n",
    "# return=200.000]\n",
    "# Iteration 9: 100%|██████████| 100/100 [00:22<00:00,  4.14it/s, episode=1000, \n",
    "# return=186.100]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在CartPole-v0环境中，满分就是200分，我们发现 REINFORCE 算法效果很好，可以达到200分。接下来我们绘制训练过程中每一条轨迹的回报变化图。由于回报抖动比较大，往往会进行平滑处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes_list = list(range(len(return_list)))\n",
    "plt.plot(episodes_list,return_list)\n",
    "plt.xlabel('Episodes')   \n",
    "plt.ylabel('Returns')  \n",
    "plt.title('REINFORCE on {}'.format(env_name))  \n",
    "plt.show()\n",
    "\n",
    "mv_return = rl_utils.moving_average(return_list, 9)\n",
    "plt.plot(episodes_list, mv_return)\n",
    "plt.xlabel('Episodes')   \n",
    "plt.ylabel('Returns')  \n",
    "plt.title('REINFORCE on {}'.format(env_name))  \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，随着收集到的轨迹越来越多，REINFORCE 算法有效地学习到了最优策略。不过，相比于前面的 DQN 算法，REINFORCE 算法使用了更多的序列，这是因为 REINFORCE 算法是一个在线策略算法，之前收集到的轨迹数据不会被再次利用。此外，REINFORCE 算法的性能也有一定程度的波动，这主要是因为每条采样轨迹的回报值波动比较大，这也是 REINFORCE 算法主要的不足。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.5  小结\n",
    "REINFORCE 算法是策略梯度乃至强化学习的典型代表，智能体根据当前策略直接和环境交互，通过采样得到的轨迹数据直接计算出策略参数的梯度，进而更新当前策略，使其向最大化策略期望回报的目标靠近。这种学习方式是典型的从交互中学习，并且其优化的目标（即策略期望回报）正是最终所使用策略的性能，这比基于价值的强化学习算法的优化目标（一般是时序差分误差的最小化）要更加直接。\n",
    "REINFORCE 算法理论上是能保证局部最优的，它实际上是借助蒙特卡洛方法采样轨迹来估计动作价值，这种做法的一大优点是可以得到无偏的梯度。但是，正是因为使用了蒙特卡洛方法，REINFORCE 算法的梯度估计的方差很大，可能会造成一定程度上的不稳定，这也是第10章将介绍的 Actor-Critic 算法要解决的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.6 扩展阅读：策略梯度证明\n",
    "策略梯度定理是强化学习中的重要理论。本节我们来证明$\\nabla_{\\theta}J(\\theta) \\propto \\sum_{s \\in S}\\nu^{\\pi_{\\theta}}(s)\\sum_{a \\in A}Q^{\\pi_\\theta}(s,a)\\nabla_{\\theta}\\pi_{\\theta}(a|s)$。\n",
    "                        \n",
    "先从状态价值函数的推导开始：\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta}V^{\\pi_\\theta}(s) &=\\nabla_{\\theta}(\\sum_{a \\in A} \\pi_{\\theta}(a|s)Q^{\\pi_\\theta}(s,a)) \\\\\n",
    "&=\\sum_{a\\in A}(\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi_\\theta}(s,a) + \\pi_{\\theta}(a|s)\\nabla_{\\theta}Q^{\\pi_\\theta}(s,a))\\\\\n",
    "&=\\sum_{a\\in A}(\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi_\\theta}(s,a) + \\pi_{\\theta}(a|s)\\nabla_{\\theta}\\sum_{s',r}p(s',r|s,a)(r+\\gamma V^{\\pi_\\theta}(s'))\\\\\n",
    "&=\\sum_{a\\in A}(\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi_\\theta}(s,a) + \\gamma \\pi_{\\theta}(a|s)\\sum_{s',r}p(s',r|s,a)\\nabla_{\\theta}V^{\\pi_\\theta}(s'))\\\\\n",
    "&=\\sum_{a\\in A}(\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi_\\theta}(s,a) + \\gamma \\pi_{\\theta}(a|s)\\sum_{s'}p(s'|s,a)\\nabla_{\\theta}V^{\\pi_\\theta}(s'))\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "为了简化表示，我们让$\\phi(s)=\\sum_{a \\in A}\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi_\\theta}(s,a)$, 定义$d^{\\pi_\\theta}(s\\rightarrow x, k)$为策略$\\pi$从状态$s$出发$k$步后到达状态$x$的概率。我们继续推导：\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta}V^{\\pi_\\theta}(s) &= \\phi(s) + \\gamma \\sum_{a}\\pi_{\\theta}(a|s)\\sum_{s'}P(s'|s,a)\\nabla_{\\theta}V^{\\pi_\\theta}(s')\\\\\n",
    "&= \\phi(s) + \\gamma \\sum_{a}\\sum_{s'}\\pi_{\\theta}(a|s)P(s'|s,a)\\nabla_{\\theta}V^{\\pi_\\theta}(s')\\\\\n",
    "&= \\phi(s) + \\gamma \\sum_{s'}d^{\\pi_\\theta}(s \\rightarrow s',1)\\nabla_{\\theta}V^{\\pi_\\theta}(s')\\\\\n",
    "&= \\phi(s) + \\gamma \\sum_{s'}d^{\\pi_\\theta}(s \\rightarrow s',1)[\\phi(s') + \\gamma \\sum_{s''}d^{\\pi_\\theta}(s' \\rightarrow s'',1)\\nabla_{\\theta}V^{\\pi_\\theta}(s'')]\\\\\n",
    "&= \\phi(s) + \\gamma \\sum_{s'}d^{\\pi_\\theta}(s \\rightarrow s',1)\\phi(s') + \\gamma^2 \\sum_{s''}d^{\\pi_\\theta}(s \\rightarrow s'',2)\\nabla_{\\theta}V^{\\pi_\\theta}(s'')\\\\\n",
    "&= \\phi(s) + \\gamma \\sum_{s'}d^{\\pi_\\theta}(s \\rightarrow s',1)\\phi(s') + \\gamma^2 \\sum_{s''}d^{\\pi_\\theta}(s' \\rightarrow s'',2)\\phi(s'') + \\gamma^3 \\sum_{s'''}d^{\\pi_\\theta}(s \\rightarrow s''',3)\\nabla_{\\theta}V^{\\pi_\\theta}(s''')\\\\\n",
    "&= \\cdots \\\\\n",
    "&= \\sum_{x \\in S}\\sum^{\\infty}_{k=0} \\gamma^k d^{\\pi_\\theta}(s \\rightarrow x, k)\\phi(x)\n",
    "\\end{align*}\n",
    "$$\n",
    "定义$\\eta(s)= \\mathbb{E}_{s_0}[\\sum^{\\infty}_{k=0} \\gamma^k d^{\\pi_\\theta}(s_{0} \\rightarrow s, k)]$。至此，回到目标函数：\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta}J(\\theta) &= \\nabla_{\\theta}\\mathbb{E}_{s_0}[V^{\\pi_\\theta}(s_{0})]\\\\\n",
    "&= \\sum_{s} \\mathbb{E}_{s_0}\\left[ \\sum^{\\infty}_{k=0} \\gamma^k d^{\\pi_\\theta}(s_{0} \\rightarrow s, k)\\right]\\phi(s)\\\\\n",
    "&= \\sum_{s}\\eta(s)\\phi(s)\\\\\n",
    "&= \\left(\\sum_{s}\\eta(s)\\right)\\sum_{s}\\frac{\\eta(s)}{\\sum_{s}\\eta(s)}\\phi(s)\\\\\n",
    "&\\propto \\sum_{s}\\frac{\\eta(s)}{\\sum_{s}\\eta(s)}\\phi(s)\\\\\n",
    "&= \\sum_{s}\\nu^{\\pi_{\\theta}}(s)\\sum_{a}Q^{\\pi_\\theta}(s,a)\\nabla_{\\theta}\\pi_{\\theta}(a|s)\n",
    "\\end{align*}\n",
    "$$\n",
    "证明完毕！"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.7  参考文献\n",
    "[1]\tSUTTON R S, MCALLESTER D A, SINGH S P, et al. Policy gradient methods for reinforcement learning with function approximation [C] // Advances in neural information processing systems, 2000: 1057-1063. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
